{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgjang/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from models.resnet_32x32 import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
    "from models.densenet import DenseNet121, DenseNet161, DenseNet169, DenseNet201\n",
    "from models.dla import DLA\n",
    "from utils.base_utils import get_data_loader, AverageMeter, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch GAN Training')\n",
    "    parser.add_argument('--attacker',\n",
    "                        default='GMI',\n",
    "                        type=str,\n",
    "                        help='GMI | GANMI | VMI | PMI')\n",
    "    parser.add_argument('--target_dataset', default='mnist', type=str, help='mnist | cifar10 | lsun | imagenet | folder | lfw | fake')\n",
    "    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n",
    "    parser.add_argument('--num-workers', type=int, default=4, help='number of workers')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    if args.attacker == 'GMI':\n",
    "        args.attacker_name = 'General_MI'\n",
    "    elif args.attacker == 'GANMI':\n",
    "        args.attacker_name = 'Generative_MI'\n",
    "    elif args.attacker == 'VMI':\n",
    "        args.attacker_name = 'Variational_MI'\n",
    "    elif args.attacker == 'PMI':\n",
    "        args.attacker_name = 'Patch_MI'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if args.target_dataset == 'mnist':\n",
    "        args.num_channel = 1\n",
    "        args.img_size = 32\n",
    "        args.num_classes = 10\n",
    "    elif args.target_dataset == 'emnist':\n",
    "        args.num_channel = 1\n",
    "        args.img_size = 32\n",
    "        args.num_classes = 26\n",
    "    elif args.target_dataset == 'cifar10':\n",
    "        args.num_channel = 3\n",
    "        args.img_size = 32\n",
    "        args.num_classes = 10\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(args, dataset, model):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=args.batch_size, \n",
    "                             shuffle=False, \n",
    "                             num_workers=args.num_workers)\n",
    "    ACC_t1 = AverageMeter()\n",
    "    Conf = AverageMeter()\n",
    "    ACC_t5 = AverageMeter()\n",
    "    for step, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "        outputs = model(inputs)\n",
    "        top1, top5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "        confidence = outputs.softmax(1)[range(outputs.size(0)), targets].mean()\n",
    "        ACC_t1.update(top1.item(), inputs.size(0))\n",
    "        ACC_t5.update(top5.item(), inputs.size(0))\n",
    "        Conf.update(confidence.item(), inputs.size(0))\n",
    "    return ACC_t1.avg, ACC_t5.avg, Conf.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_fid.fid_score\n",
    "import torch\n",
    "from pytorch_fid.inception import InceptionV3\n",
    "\n",
    "def SingleClassSubset(dataset, cls):\n",
    "    indices = np.where(np.array(dataset.targets) == cls)[0]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "class PRCD:\n",
    "    def __init__(self, args, dataset_real, dataset_fake):\n",
    "        self.dataset_real = dataset_real\n",
    "        self.dataset_fake = dataset_fake\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dims = 2048\n",
    "        self.num_workers = args.num_workers\n",
    "        self.device = args.device\n",
    "        self.num_classes = args.num_classes\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[self.dims]\n",
    "        inception_model = InceptionV3([block_idx])\n",
    "        self.inception_model = inception_model.to(self.device)\n",
    "        self.up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=True).to(self.device)\n",
    "    def compute_metric(self, k=3):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        density_list = []\n",
    "        coverage_list = []\n",
    "        for step, cls in tqdm(enumerate(range(self.num_classes))):\n",
    "            with torch.no_grad():\n",
    "                embedding_fake = self.compute_embedding(self.dataset_fake, cls)\n",
    "                embedding_real = self.compute_embedding(self.dataset_real, cls)\n",
    "                length = min(embedding_fake.shape[0], embedding_real.shape[0])\n",
    "                embedding_fake = embedding_fake[:length]\n",
    "                embedding_real = embedding_real[:length]\n",
    "                \n",
    "                pair_dist_real = torch.cdist(embedding_real, embedding_real, p=2)\n",
    "                pair_dist_real = torch.sort(pair_dist_real, dim=1, descending=False)[0]\n",
    "                pair_dist_fake = torch.cdist(embedding_fake, embedding_fake, p=2)\n",
    "                pair_dist_fake = torch.sort(pair_dist_fake, dim=1, descending=False)[0]\n",
    "                radius_real = pair_dist_real[:, k]\n",
    "                radius_fake = pair_dist_fake[:, k]\n",
    "\n",
    "                # Compute precision\n",
    "                distances_fake_to_real = torch.cdist(embedding_fake, embedding_real, p=2)\n",
    "                min_dist_fake_to_real, nn_real = distances_fake_to_real.min(dim=1)\n",
    "                precision = (min_dist_fake_to_real <= radius_real[nn_real]).float().mean()\n",
    "                precision_list.append(precision.cpu().item())\n",
    "\n",
    "                # Compute recall\n",
    "                distances_real_to_fake = torch.cdist(embedding_real, embedding_fake, p=2)\n",
    "                min_dist_real_to_fake, nn_fake = distances_real_to_fake.min(dim=1)\n",
    "                recall = (min_dist_real_to_fake <= radius_fake[nn_fake]).float().mean()\n",
    "                recall_list.append(recall.cpu().item())\n",
    "\n",
    "                # Compute density\n",
    "                num_samples = distances_fake_to_real.shape[0]\n",
    "                sphere_counter = (distances_fake_to_real <= radius_real.repeat(num_samples, 1)).float().sum(dim=0).mean()\n",
    "                density = sphere_counter / k\n",
    "                density_list.append(density.cpu().item())\n",
    "\n",
    "                # Compute coverage\n",
    "                num_neighbors = (distances_fake_to_real <= radius_real.repeat(num_samples, 1)).float().sum(dim=0)\n",
    "                coverage = (num_neighbors > 0).float().mean()\n",
    "                coverage_list.append(coverage.cpu().item())\n",
    "\n",
    "        # Compute mean over targets\n",
    "        precision = np.mean(precision_list)\n",
    "        recall = np.mean(recall_list)\n",
    "        density = np.mean(density_list)\n",
    "        coverage = np.mean(coverage_list)\n",
    "        return precision, recall, density, coverage\n",
    "\n",
    "    def compute_embedding(self, dataset, cls=None):\n",
    "        self.inception_model.eval()\n",
    "        if cls is not None:\n",
    "            dataset = SingleClassSubset(dataset, cls)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                 batch_size=self.batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 drop_last=False,\n",
    "                                                 pin_memory=True,\n",
    "                                                 num_workers=self.num_workers)\n",
    "        pred_arr = np.empty((len(dataset), self.dims))\n",
    "        start_idx = 0\n",
    "        max_iter = int(len(dataset) / self.batch_size)\n",
    "        for step, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(self.device)\n",
    "            if x.shape[1] == 1:\n",
    "                x = x.repeat(1, 3, 1, 1)\n",
    "            x = self.up(x)\n",
    "            pred = self.inception_model(x)[0]\n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr[start_idx:start_idx + pred.shape[0]] = pred\n",
    "            start_idx = start_idx + pred.shape[0]\n",
    "\n",
    "        return torch.from_numpy(pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FID_Score:\n",
    "    def __init__(self, args, dataset_1, dataset_2):\n",
    "        self.dataset_1 = dataset_1\n",
    "        self.dataset_2 = dataset_2\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dims = 2048\n",
    "        self.num_workers = args.num_workers\n",
    "        self.device = args.device\n",
    "        self.num_classes = args.num_classes\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[self.dims]\n",
    "        inception_model = InceptionV3([block_idx])\n",
    "        self.inception_model = inception_model.to(self.device)\n",
    "        self.up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=True).to(self.device)\n",
    "            \n",
    "    def compute_fid(self):\n",
    "        m1, s1 = self.compute_statistics(self.dataset_1)\n",
    "        m2, s2 = self.compute_statistics(self.dataset_2)\n",
    "        fid_value = pytorch_fid.fid_score.calculate_frechet_distance(\n",
    "            m1, s1, m2, s2)\n",
    "        return fid_value\n",
    "\n",
    "    def compute_statistics(self, dataset):\n",
    "        self.inception_model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                 batch_size=self.batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 drop_last=False,\n",
    "                                                 pin_memory=True,\n",
    "                                                 num_workers=self.num_workers)\n",
    "        pred_arr = np.empty((len(dataset), self.dims))\n",
    "        start_idx = 0\n",
    "        max_iter = int(len(dataset) / self.batch_size)\n",
    "        for step, (x, y) in tqdm(enumerate(dataloader)):\n",
    "            with torch.no_grad():\n",
    "                x = x.to(self.device)\n",
    "                if x.shape[1] == 1:\n",
    "                    x = x.repeat(1, 3, 1, 1)\n",
    "                x = self.up(x)\n",
    "                pred = self.inception_model(x)[0]\n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr[start_idx:start_idx + pred.shape[0]] = pred\n",
    "            start_idx = start_idx + pred.shape[0]\n",
    "\n",
    "        mu = np.mean(pred_arr, axis=0)\n",
    "        sigma = np.cov(pred_arr, rowvar=False)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparser()\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args.img_size, args.img_size)),\n",
    "    transforms.Grayscale(1) if args.num_channel == 1 else transforms.Resize((args.img_size, args.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) if args.num_channel == 3 else transforms.Normalize((0.5,),(0.5,))\n",
    "    ])\n",
    "os.chdir('/home/jgjang/Patch-Attack/PatAtt_v4/')\n",
    "if args.target_dataset == 'mnist':\n",
    "    target_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "elif args.target_dataset == 'emnist':   \n",
    "    target_dataset = datasets.EMNIST('../data', train=True, download=True, transform=transform)\n",
    "elif args.target_dataset == 'cifar10':\n",
    "    target_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "target_classifier = DLA(num_classes = args.num_classes, num_channel = args.num_channel).to(args.device)\n",
    "target_classifier.load_state_dict(torch.load(f'../experiments/classifier/{args.target_dataset}_valid/best.pt')['model'])\n",
    "attack_dataset = datasets.ImageFolder(f'./Results/{args.attacker_name}/{args.target_dataset}', transform=transform)\n",
    "# compute variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Top1: 0.4589 | Accuracy Top5: 0.9085 | Confidence: 0.4320\n"
     ]
    }
   ],
   "source": [
    "Acc_t1, Acc_t5, Confidence = acc_metric(args, attack_dataset, target_classifier)\n",
    "print(f'Accuracy Top1: {Acc_t1:.4f} | Accuracy Top5: {Acc_t5:.4f} | Confidence: {Confidence:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "prcd = PRCD(args, target_dataset, attack_dataset)\n",
    "Precision, Recall, Coverage, Density = prcd.compute_metric()\n",
    "print(f'Precision: {Precision:.4f} | Recall: {Recall:.4f} | Coverage: {Coverage:.4f} | Density: {Density:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:55, 34.07it/s]\n",
      "313it [00:19, 16.32it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fid \u001b[39m=\u001b[39m FID_Score(args,target_dataset, attack_dataset)\n\u001b[1;32m      2\u001b[0m fid_score \u001b[39m=\u001b[39m fid\u001b[39m.\u001b[39mcompute_fid()\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFID: \u001b[39m\u001b[39m{\u001b[39;00mFID\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FID' is not defined"
     ]
    }
   ],
   "source": [
    "fid = FID_Score(args,target_dataset, attack_dataset)\n",
    "fid_score = fid.compute_fid()\n",
    "print(f'FID: {fid_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in 1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
